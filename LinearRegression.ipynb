{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/da1/fenxi/install/anaconda2/envs/python3tf/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v1 : 手工定义w, b\n",
    "### 这里使用Adam，因为使用GradientDescentOptimizer会梯度爆炸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_regression_v1(train_x,train_y,epoch=20000,lr=0.001):\n",
    "    with tf.variable_scope(\"v1\", reuse=tf.AUTO_REUSE) as scope:\n",
    "        x = tf.placeholder(tf.float32)\n",
    "        y = tf.placeholder(tf.float32)\n",
    "        w = tf.Variable(tf.random_normal([1]))\n",
    "        b = tf.Variable(tf.random_normal([1]))\n",
    "\n",
    "        pred = tf.add(tf.multiply(x, w), b)\n",
    "        loss = tf.reduce_sum(tf.pow(pred - y, 2))\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for i in range(1,epoch+1):\n",
    "                sess.run(optimizer, {x:train_x, y:train_y})\n",
    "                if i % 2000 == 0:\n",
    "                    ww, bb, ll = sess.run([w, b, loss], {x:train_x, y:train_y})\n",
    "#                     print(\"%s, %s, %s, %s\" % (i, ww, bb, ll))\n",
    "                    print(\"%s, %s\" % (i, ll))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v2 : 使用梯度裁剪解决SGD梯度爆炸的问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_regression_v2(train_x,train_y,epoch=20000,lr=0.001):\n",
    "    with tf.variable_scope(\"v2\", reuse=tf.AUTO_REUSE) as scope:\n",
    "        x = tf.placeholder(tf.float32, [100,1])\n",
    "        y = tf.placeholder(tf.float32, [100,1])\n",
    "        w = tf.Variable(tf.random_normal([1]))\n",
    "        b = tf.Variable(tf.random_normal([1]))\n",
    "\n",
    "        pred = tf.add(tf.multiply(x, w), b)\n",
    "        loss = tf.reduce_sum(tf.pow(pred - y, 2))\n",
    "        \"\"\" 梯度裁剪 \"\"\"\n",
    "        op = tf.train.GradientDescentOptimizer(lr)\n",
    "        grads_and_vars = op.compute_gradients(loss)\n",
    "        grad, variables = zip(*grads_and_vars)\n",
    "        clipped_grad, glob_norm = tf.clip_by_global_norm(grad, clip_norm=5)\n",
    "        optimizer = op.apply_gradients(zip(clipped_grad, variables))\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for i in range(1,epoch+1):\n",
    "                sess.run(optimizer, {x:train_x, y:train_y})\n",
    "                if i % 2000 == 0:\n",
    "                    ww, bb, ll = sess.run([w, b, loss], {x:train_x, y:train_y})\n",
    "                    print(\"%s, %s\" % (i, ll))\n",
    "#                     print(\"%s, %s, %s, %s\" % (i, ww, bb, ll))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v3：使用dense构造全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_regression_v3(train_x,train_y,epoch=20000,lr=0.001):\n",
    "    with tf.variable_scope(\"v2\", reuse=tf.AUTO_REUSE) as scope:\n",
    "        x = tf.placeholder(tf.float32, [100,1])\n",
    "        y = tf.placeholder(tf.float32, [100,1])\n",
    "        pred = tf.layers.dense(x, 1, use_bias=True)\n",
    "        loss = tf.reduce_sum(tf.pow(pred - y, 2),name=\"loss\")\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for i in range(1,epoch+1):\n",
    "                sess.run(optimizer, {x:train_x, y:train_y})\n",
    "                if i % 2000 == 0:\n",
    "                    ll = sess.run([loss], {x:train_x, y:train_y})\n",
    "                    print(\"%s, %s\" % (i, ll))\n",
    "#             gv = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='v2')\n",
    "#             for var in gv:\n",
    "#                 print(\"%s : %s\" % (var.name, sess.run(var, {x:train_x, y:train_y})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = np.arange(100).reshape([100,1])\n",
    "train_y = train_x * 2.0 - 1.5 + 0.05 * np.random.randn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000, 1038.709\n",
      "4000, 110.91292\n",
      "6000, 85.89453\n",
      "8000, 44.006004\n",
      "10000, 8.602722\n",
      "12000, 0.15436323\n",
      "14000, 2.8198938e-06\n",
      "16000, 6.949996e-12\n",
      "18000, 6.949996e-12\n",
      "20000, 6.949996e-12\n"
     ]
    }
   ],
   "source": [
    "linear_regression_v1(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000, 49.28425\n",
      "4000, 27.510662\n",
      "6000, 15.776136\n",
      "8000, 9.45639\n",
      "10000, 6.044359\n",
      "12000, 4.2093344\n",
      "14000, 3.2350032\n",
      "16000, 2.6731892\n",
      "18000, 2.3982048\n",
      "20000, 2.268446\n"
     ]
    }
   ],
   "source": [
    "linear_regression_v2(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000, [1128122.8]\n",
      "4000, [116878.73]\n",
      "6000, [1304.0945]\n",
      "8000, [594.8844]\n",
      "10000, [494.91302]\n",
      "12000, [317.5016]\n",
      "14000, [131.33923]\n",
      "16000, [26.674183]\n",
      "18000, [0.9867292]\n",
      "20000, [0.00019921354]\n"
     ]
    }
   ],
   "source": [
    "linear_regression_v3(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
